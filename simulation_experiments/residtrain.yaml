# env
env: drq-robosuite
action_repeat: 1 #not used for now 
# train
num_train_steps: 500000
num_train_iters: 1
num_seed_steps: 500
replay_buffer_capacity: 60000 
seed: ??
# eval
eval_frequency: 1000
num_eval_episodes: 20
# misc
log_frequency_step: 500
log_save_tb: true
save_video: true
device: cuda
# observation
image_size: 84
image_pad: 4
horizon: 500
system: sim 

use_lowdim: true
# global params
lr: 1e-3
# IMPORTANT: please use a batch size of 512 to reproduce the results in the paper. Hovewer, with a smaller batch size it still works well.


##### MODE OF OPERATION ######
eval_only: false
load_dir: ../../../BlockedPickPlace_residual/2021.10.18/130121_seed=20/245000_

# things that we can't change without breaking something 
lowdim_stack: 10
frame_stack: 3
use_squashed: True #squashed vs not squashed distributions

#####RUN CHANGES HERE ############
# core adjustments 
stack: 10 
step: 100000
num_demos: 50
orig_seed: ${seed}

actor_root: /iris/u/maxjdu/communal_results/
demo_root: /iris/u/maxjdu/communal_demos/
actor_name: ${environmentName}_${num_demos}_${orig_seed}/${step}_ 
demo_file: ${environmentName}_${num_demos}_${orig_seed}/resid/demos.pkl

#logging 
runName: ${environmentName}_${num_demos}_${orig_seed}/resid/

#modality and environment
environmentName: IndicatorBoxBlock #IMPORTANT: this is the environment that is run 
modalities: [object_sound, robot0_eef_pos, robot0_gripper_qpos, robot0_gripper_qvel]
cameraName: agentview_image

#misc training parameters 
batch_size: 16
expert_workers: 4 #CHANGE BACK
memory_workers: 4 #CHANGE BACK
scale_BC: 300 #this is the intensity of the BC contribution to the training. This can be a sensitive parameter
episodeLength: 500

#####RUN END CHANGES HERE #########

#different names currently registered
#"BlockedPickPlace"
#"IndicatorBoxBlock"


#modalities to choose from
# robot0_joint_pos_cos  :  (7,)
# robot0_joint_pos_sin  :  (7,)
# robot0_joint_vel  :  (7,)
# robot0_eef_pos  :  (3,)
# robot0_eef_quat  :  (4,)
# robot0_gripper_qpos  :  (2,) #two values roughly identical but inverted. Larger number (0.038) indicates open, smaller number (~0) indicates closed
# robot0_gripper_qvel  :  (2,)
# agentview_image  :  (84, 84, 3)
# cube_pos  :  (3,)
# cube_quat  :  (4,)
# gripper_to_cube_pos  :  (3,)
# gripper_force  :  (3,)
# gripper_torque  :  (3,)
# object_sound  :  (2,) #the processed XY speed that an object moves (roughly is like sound)
# robot0_proprio-state  :  (32,) CONCATENATED from cos, sin, vel, pos, quat, qpos, qvel
# image-state  :  (84, 84, 3) DON'T USE THIS, use agentview_image instead
# object-state  :  (18,)
# robot0_gripper_act_force (2,) #don't use this 
# robot0_gripper_joint_force (2,) #finger force (not wrist force; use this one)

#sidenote: gripper is negative in the action, gripper closed is positive in the action 

# agent configuration
agent:
  name: drq
  class: core.drq_memory.DRQAgent
  params:
    obs_shape: ??? # to be specified later
    action_shape: ??? # to be specified later
    action_range: ??? # to be specified later
    lowdim_dim: ??? #to b e specified later
    device: ${device}
    encoder_cfg: ${encoder}
    critic_cfg: ${critic}
    actor_cfg: ${actor}
    discount: 0.99
    init_temperature: 0.1
    lr: ${lr}
    actor_update_frequency: 2
    critic_tau: 0.01
    critic_target_update_frequency: 2
    batch_size: ${batch_size}
    log_frequency: 20

resid_agent:
  name: drq
  class: core.drq_memory.DRQAgent
  params:
    obs_shape: ${agent.params.obs_shape} 
    action_shape: ${agent.params.action_shape}
    action_range: ${agent.params.action_range}
    lowdim_dim: ??? #to be combined later
    device: ${device}
    encoder_cfg: ${resid_encoder}
    critic_cfg: ${resid_critic}
    actor_cfg: ${resid_actor}
    discount: 0.99
    init_temperature: 0.1
    lr: ${lr}
    actor_update_frequency: 2
    critic_tau: 0.01
    critic_target_update_frequency: 2
    batch_size: ${batch_size}
    log_frequency: 20

critic:
  class: core.drq_memory.Critic
  params:
    encoder_cfg: ${agent.params.encoder_cfg}
    action_shape: ${agent.params.action_shape}
    hidden_dim: 1024
    hidden_depth: 2

actor:
  class: core.drq_memory.Actor
  params:
    encoder_cfg: ${agent.params.encoder_cfg}
    action_shape: ${agent.params.action_shape}
    hidden_depth: 2
    hidden_dim: 1024
    log_std_bounds: [-10, 2]

resid_critic:
  class: core.drq_memory.Critic
  params:
    encoder_cfg: ${resid_agent.params.encoder_cfg}
    action_shape: ${agent.params.action_shape}
    hidden_dim: 1024
    hidden_depth: 2

resid_actor:
  class: core.drq_memory.ActorResidual
  params:
    encoder_cfg: ${resid_agent.params.encoder_cfg}
    action_shape: ${agent.params.action_shape}
    hidden_depth: 2
    hidden_dim: 1024
    log_std_bounds: [-10, 2]

encoder:
  class: core.drq_memory.Encoder
  params:
      obs_shape: ${agent.params.obs_shape}
      feature_dim: 50
      lowdim_dim: ${agent.params.lowdim_dim}
      num_layers: 4
      num_filters: 32
      output_dim: 35
      output_logits: False

resid_encoder:
  class: core.drq_memory.Encoder
  params:
      obs_shape: ${resid_agent.params.obs_shape}
      feature_dim: 50
      lowdim_dim: ${resid_agent.params.lowdim_dim}
      num_layers: 4
      num_filters: 32
      output_dim: 35
      output_logits: False

# get_out: ../../../../ #to get out of the original logging directory 

log_dir: /iris/u/maxjdu/communal_results/${runName}
# hydra configuration
hydra: 
  name: ${env}
  run:
    dir: ${log_dir}
#     dir: ./runs/${runName}/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}
